#!/usr/bin/env python3
"""
EDINET Data Consolidation Tool

Reads multiple JSON files generated by fetch_edinet_financial_documents.py,
consolidates data from the past year, handles duplicate companies by keeping
the latest data, and outputs consolidated data.

Usage:
    python bin/consolidate_documents.py --inputdir data/jsons/ --output data/consolidated.json
"""

import argparse
import json
import os
import sys
import glob
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from collections import defaultdict

# Add parent directory to path to access lib module
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from lib.edinet_common import setup_logging, ensure_output_directory


class DataConsolidator:
    """Consolidates financial data from multiple JSON files"""
    
    def __init__(self, input_directory: str):
        self.input_directory = input_directory
        self.consolidated_data = {}
        self.logger = logging.getLogger(__name__)
        
    def consolidate_files(self) -> List[Dict[str, Any]]:
        """
        Consolidate all JSON files in the input directory
        
        Returns:
            List of consolidated company financial data
        """
        self.logger.info(f"Scanning directory: {self.input_directory}")
        
        # Find all JSON files in the directory
        json_files = glob.glob(os.path.join(self.input_directory, "*.json"))
        
        if not json_files:
            self.logger.warning("No JSON files found in the input directory")
            return []
        
        # Filter files to only include those from the past 400 days
        filtered_files = self._filter_files_by_date(json_files)
        
        if not filtered_files:
            self.logger.warning("No JSON files found within the past 400 days")
            return []
        
        self.logger.info(f"Found {len(json_files)} JSON files, filtered to {len(filtered_files)} files within past 400 days")
        
        # Process each JSON file
        company_data = defaultdict(list)
        
        for json_file in filtered_files:
            self.logger.debug(f"Processing: {os.path.basename(json_file)}")
            
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # Add each company's data to the collection
                companies_in_file = 0
                for company in data:
                    if isinstance(company, dict) and 'secCode' in company:
                        sec_code = company['secCode']
                        company_data[sec_code].append(company)
                        companies_in_file += 1
                
                self.logger.debug(f"  Found {companies_in_file} companies in {os.path.basename(json_file)}")
                        
            except json.JSONDecodeError as e:
                self.logger.error(f"Error parsing {json_file}: {e}")
                continue
            except Exception as e:
                self.logger.error(f"Error reading {json_file}: {e}")
                continue
        
        # Handle duplicates by keeping the latest data for each company
        consolidated_companies = []
        
        for sec_code, company_entries in company_data.items():
            if not company_entries:
                continue
            
            # Sort by retrievedDate to get the latest entry
            latest_entry = self._get_latest_entry(company_entries)
            
            if latest_entry:
                consolidated_companies.append(latest_entry)
                if len(company_entries) > 1:
                    self.logger.debug(f"  Consolidated {len(company_entries)} entries for company {sec_code}")
        
        self.logger.info(f"Consolidation complete: {len(consolidated_companies)} unique companies")
        return consolidated_companies
    
    def _filter_files_by_date(self, json_files: List[str]) -> List[str]:
        """
        Filter JSON files to only include those from the past 400 days
        
        Args:
            json_files: List of JSON file paths
            
        Returns:
            List of filtered JSON file paths
        """
        current_date = datetime.now()
        cutoff_date = current_date - timedelta(days=400)  # Past 400 days
        
        filtered_files = []
        
        for json_file in json_files:
            filename = os.path.basename(json_file)
            
            # Extract date from filename (expected format: YYYY-MM-DD.json)
            try:
                # Remove .json extension and parse date
                date_str = filename.replace('.json', '')
                file_date = datetime.strptime(date_str, '%Y-%m-%d')
                
                if file_date >= cutoff_date:
                    filtered_files.append(json_file)
                else:
                    self.logger.debug(f"Excluding file older than 400 days: {filename}")
                    
            except ValueError:
                # If filename doesn't match expected format, include it anyway
                self.logger.warning(f"Could not parse date from filename: {filename}, including in processing")
                filtered_files.append(json_file)
        
        # Log the oldest file being processed
        if filtered_files:
            oldest_file = self._get_oldest_file(filtered_files)
            if oldest_file:
                self.logger.info(f"Oldest file being processed: {os.path.basename(oldest_file)}")
        
        return filtered_files
    
    def _get_oldest_file(self, json_files: List[str]) -> Optional[str]:
        """
        Get the oldest file from the list based on filename date
        
        Args:
            json_files: List of JSON file paths
            
        Returns:
            Path to oldest file or None if no valid files
        """
        oldest_file = None
        oldest_date = None
        
        for json_file in json_files:
            filename = os.path.basename(json_file)
            
            try:
                # Remove .json extension and parse date  
                date_str = filename.replace('.json', '')
                file_date = datetime.strptime(date_str, '%Y-%m-%d')
                
                if oldest_date is None or file_date < oldest_date:
                    oldest_date = file_date
                    oldest_file = json_file
                    
            except ValueError:
                # Skip files that don't match expected format
                continue
        
        return oldest_file
    
    def _get_latest_entry(self, entries: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """
        Get the latest entry based on retrievedDate
        
        Args:
            entries: List of company data entries
            
        Returns:
            Latest entry or None if no valid entries
        """
        if not entries:
            return None
            
        # Sort entries by retrievedDate (latest first)
        try:
            sorted_entries = sorted(
                entries,
                key=lambda x: datetime.strptime(x.get('retrievedDate', '1900-01-01'), '%Y-%m-%d'),
                reverse=True
            )
            return sorted_entries[0]
            
        except (ValueError, TypeError) as e:
            self.logger.warning(f"Error sorting entries by date: {e}")
            # Return the first entry if date parsing fails
            return entries[0]
    
    def save_consolidated_data(self, consolidated_data: List[Dict[str, Any]], output_file: str) -> bool:
        """
        Save consolidated data to JSON file
        
        Args:
            consolidated_data: List of consolidated company data
            output_file: Output file path
            
        Returns:
            True if successful, False otherwise
        """
        try:
            # Ensure output directory exists
            if not ensure_output_directory(output_file):
                raise Exception("Failed to create output directory")
            
            # Sort companies by securities code for consistent output
            sorted_data = sorted(consolidated_data, key=lambda x: x.get('secCode') or '')
            
            # Write to JSON file with proper formatting
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(sorted_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"Consolidated data saved to: {output_file}")
            return True
            
        except Exception as e:
            self.logger.error(f"Error saving consolidated data: {e}")
            return False
    
    def generate_summary_report(self, consolidated_data: List[Dict[str, Any]]) -> None:
        """
        Generate a summary report of the consolidated data
        
        Args:
            consolidated_data: List of consolidated company data
        """
        if not consolidated_data:
            print("No data to summarize")
            return
        
        print("\n=== Consolidation Summary ===")
        print(f"Total companies: {len(consolidated_data)}")
        
        # Count companies by retrieval date
        date_counts = defaultdict(int)
        companies_with_data = 0
        
        for company in consolidated_data:
            retrieved_date = company.get('retrievedDate', 'Unknown')
            date_counts[retrieved_date] += 1
            
            # Count companies with actual financial data
            if any(company.get(field) is not None for field in ['netSales', 'operatingIncome', 'employees']):
                companies_with_data += 1
        
        print(f"Companies with financial data: {companies_with_data}")
        print(f"Companies by retrieval date:")
        
        for date, count in sorted(date_counts.items()):
            print(f"  {date}: {count} companies")
        
        # Sample of company codes
        sample_codes = [company.get('secCode') or 'Unknown' for company in consolidated_data[:10]]
        print(f"Sample company codes: {', '.join(sample_codes)}")
        
        print("=" * 30)


class InputValidator:
    """Validates input directories and files"""
    
    @staticmethod
    def validate_input_directory(directory: str) -> bool:
        """
        Validate that the input directory exists and contains JSON files
        
        Args:
            directory: Path to input directory
            
        Returns:
            True if valid, False otherwise
        """
        if not os.path.exists(directory):
            print(f"Error: Input directory does not exist: {directory}", file=sys.stderr)
            return False
        
        if not os.path.isdir(directory):
            print(f"Error: Input path is not a directory: {directory}", file=sys.stderr)
            return False
        
        # Check for JSON files
        json_files = glob.glob(os.path.join(directory, "*.json"))
        if not json_files:
            print(f"Warning: No JSON files found in directory: {directory}", file=sys.stderr)
        
        return True


def main():
    """Main entry point for data consolidation"""
    parser = argparse.ArgumentParser(description="Consolidate financial data from multiple JSON files")
    parser.add_argument("--inputdir", required=True, help="Input directory containing JSON files")
    parser.add_argument("--output", required=True, help="Output file path")
    parser.add_argument("--summary", action="store_true", help="Display summary report")
    parser.add_argument("--verbose", "-v", action="store_true", help="Enable verbose logging")
    
    args = parser.parse_args()
    
    # Setup logging
    logger = setup_logging('consolidate_documents', args.verbose)
    
    logger.info(f"Starting data consolidation from: {args.inputdir}")
    
    try:
        # Validate input directory
        validator = InputValidator()
        if not validator.validate_input_directory(args.inputdir):
            logger.error("Input directory validation failed")
            sys.exit(1)
        
        # Initialize consolidator
        consolidator = DataConsolidator(args.inputdir)
        
        logger.info("Starting data consolidation...")
        
        # Consolidate data from all JSON files
        consolidated_data = consolidator.consolidate_files()
        
        if not consolidated_data:
            logger.warning("No data to consolidate")
            sys.exit(1)
        
        logger.info(f"Consolidated {len(consolidated_data)} companies")
        
        # Save consolidated data
        success = consolidator.save_consolidated_data(consolidated_data, args.output)
        
        if not success:
            logger.error("Failed to save consolidated data")
            sys.exit(1)
        
        # Generate summary report if requested
        if args.summary:
            consolidator.generate_summary_report(consolidated_data)
        
        logger.info("Consolidation completed successfully!")
        logger.info(f"Output file: {args.output}")
        logger.info(f"Total companies: {len(consolidated_data)}")
        
    except KeyboardInterrupt:
        logger.info("Consolidation interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Unexpected error during consolidation: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()