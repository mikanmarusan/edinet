#!/usr/bin/env python3
"""
EDINET Data Consolidation Tool

Reads multiple JSON files generated by Tool 1, consolidates data from the past year,
handles duplicate companies by keeping the latest data, and outputs consolidated data.

Usage:
    python tool2.py --inputdir data/jsons/
"""

import argparse
import json
import os
import sys
import glob
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
from collections import defaultdict


class DataConsolidator:
    """Consolidates financial data from multiple JSON files"""
    
    def __init__(self, input_directory: str):
        self.input_directory = input_directory
        self.consolidated_data = {}
        self.logger = logging.getLogger(__name__)
        
    def consolidate_files(self) -> List[Dict[str, Any]]:
        """
        Consolidate all JSON files in the input directory
        
        Returns:
            List of consolidated company financial data
        """
        self.logger.info(f"Scanning directory: {self.input_directory}")
        
        # Find all JSON files in the directory
        json_files = glob.glob(os.path.join(self.input_directory, "*.json"))
        
        if not json_files:
            self.logger.warning("No JSON files found in the input directory")
            return []
        
        self.logger.info(f"Found {len(json_files)} JSON files")
        
        # Process each JSON file
        company_data = defaultdict(list)
        
        for json_file in json_files:
            self.logger.debug(f"Processing: {os.path.basename(json_file)}")
            
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # Add each company's data to the collection
                companies_in_file = 0
                for company in data:
                    if isinstance(company, dict) and 'secCode' in company:
                        sec_code = company['secCode']
                        company_data[sec_code].append(company)
                        companies_in_file += 1
                
                self.logger.debug(f"  Found {companies_in_file} companies in {os.path.basename(json_file)}")
                        
            except json.JSONDecodeError as e:
                self.logger.error(f"Error parsing {json_file}: {e}")
                continue
            except Exception as e:
                self.logger.error(f"Error reading {json_file}: {e}")
                continue
        
        # Handle duplicates by keeping the latest data for each company
        consolidated_companies = []
        
        for sec_code, company_entries in company_data.items():
            if not company_entries:
                continue
            
            # Sort by retrievedDate to get the latest entry
            latest_entry = self._get_latest_entry(company_entries)
            
            if latest_entry:
                consolidated_companies.append(latest_entry)
                if len(company_entries) > 1:
                    self.logger.debug(f"  Consolidated {len(company_entries)} entries for company {sec_code}")
        
        self.logger.info(f"Consolidation complete: {len(consolidated_companies)} unique companies")
        return consolidated_companies
    
    def _get_latest_entry(self, entries: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """
        Get the latest entry based on retrievedDate
        
        Args:
            entries: List of company data entries
            
        Returns:
            Latest entry or None if no valid entries
        """
        if not entries:
            return None
            
        # Sort entries by retrievedDate (latest first)
        try:
            sorted_entries = sorted(
                entries,
                key=lambda x: datetime.strptime(x.get('retrievedDate', '1900-01-01'), '%Y-%m-%d'),
                reverse=True
            )
            return sorted_entries[0]
            
        except (ValueError, TypeError) as e:
            self.logger.warning(f"Error sorting entries by date: {e}")
            # Return the first entry if date parsing fails
            return entries[0]
    
    def save_consolidated_data(self, consolidated_data: List[Dict[str, Any]], output_file: str) -> bool:
        """
        Save consolidated data to JSON file
        
        Args:
            consolidated_data: List of consolidated company data
            output_file: Output file path
            
        Returns:
            True if successful, False otherwise
        """
        try:
            # Ensure output directory exists
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            
            # Sort companies by securities code for consistent output
            sorted_data = sorted(consolidated_data, key=lambda x: x.get('secCode') or '')
            
            # Write to JSON file with proper formatting
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(sorted_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"Consolidated data saved to: {output_file}")
            return True
            
        except Exception as e:
            self.logger.error(f"Error saving consolidated data: {e}")
            return False
    
    def generate_summary_report(self, consolidated_data: List[Dict[str, Any]]) -> None:
        """
        Generate a summary report of the consolidated data
        
        Args:
            consolidated_data: List of consolidated company data
        """
        if not consolidated_data:
            print("No data to summarize")
            return
        
        print("\n=== Consolidation Summary ===")
        print(f"Total companies: {len(consolidated_data)}")
        
        # Count companies by retrieval date
        date_counts = defaultdict(int)
        companies_with_data = 0
        
        for company in consolidated_data:
            retrieved_date = company.get('retrievedDate', 'Unknown')
            date_counts[retrieved_date] += 1
            
            # Count companies with actual financial data
            if any(company.get(field) is not None for field in ['netSales', 'operatingIncome', 'employees']):
                companies_with_data += 1
        
        print(f"Companies with financial data: {companies_with_data}")
        print(f"Companies by retrieval date:")
        
        for date, count in sorted(date_counts.items()):
            print(f"  {date}: {count} companies")
        
        # Sample of company codes
        sample_codes = [company.get('secCode') or 'Unknown' for company in consolidated_data[:10]]
        print(f"Sample company codes: {', '.join(sample_codes)}")
        
        print("=" * 30)


def validate_input_directory(directory: str) -> bool:
    """
    Validate that the input directory exists and contains JSON files
    
    Args:
        directory: Path to input directory
        
    Returns:
        True if valid, False otherwise
    """
    if not os.path.exists(directory):
        print(f"Error: Input directory does not exist: {directory}", file=sys.stderr)
        return False
    
    if not os.path.isdir(directory):
        print(f"Error: Input path is not a directory: {directory}", file=sys.stderr)
        return False
    
    # Check for JSON files
    json_files = glob.glob(os.path.join(directory, "*.json"))
    if not json_files:
        print(f"Warning: No JSON files found in directory: {directory}", file=sys.stderr)
    
    return True


def setup_logging(verbose: bool = False) -> None:
    """Setup logging configuration"""
    log_level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(f'consolidate_documents_{datetime.now().strftime("%Y%m%d")}.log')
        ]
    )


def main():
    """Main entry point for tool2"""
    parser = argparse.ArgumentParser(description="Consolidate financial data from multiple JSON files")
    parser.add_argument("--inputdir", required=True, help="Input directory containing JSON files")
    parser.add_argument("--output", required=True, help="Output file path")
    parser.add_argument("--summary", action="store_true", help="Display summary report")
    parser.add_argument("--verbose", "-v", action="store_true", help="Enable verbose logging")
    
    args = parser.parse_args()
    
    # Setup logging
    setup_logging(args.verbose)
    logger = logging.getLogger(__name__)
    
    logger.info(f"Starting data consolidation from: {args.inputdir}")
    
    try:
        # Validate input directory
        if not validate_input_directory(args.inputdir):
            logger.error("Input directory validation failed")
            sys.exit(1)
        
        # Initialize consolidator
        consolidator = DataConsolidator(args.inputdir)
        
        logger.info("Starting data consolidation...")
        
        # Consolidate data from all JSON files
        consolidated_data = consolidator.consolidate_files()
        
        if not consolidated_data:
            logger.warning("No data to consolidate")
            sys.exit(1)
        
        logger.info(f"Consolidated {len(consolidated_data)} companies")
        
        # Save consolidated data
        success = consolidator.save_consolidated_data(consolidated_data, args.output)
        
        if not success:
            logger.error("Failed to save consolidated data")
            sys.exit(1)
        
        # Generate summary report if requested
        if args.summary:
            consolidator.generate_summary_report(consolidated_data)
        
        logger.info("Consolidation completed successfully!")
        logger.info(f"Output file: {args.output}")
        logger.info(f"Total companies: {len(consolidated_data)}")
        
    except KeyboardInterrupt:
        logger.info("Consolidation interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Unexpected error during consolidation: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()